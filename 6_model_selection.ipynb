{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46a9798f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Model Selection\n",
    "### `! git clone https://github.com/ds4e/model_selection`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb7aeff",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Introduction\n",
    "- Why and how do we build models?\n",
    "- There are three key pieces:\n",
    "    1. The variables in the raw data\n",
    "    2. Cleaned variables and transformations of them (the feature space) --> need to do feature modeling make model usable\n",
    "    3. The model class (hyperparameter choices: variable selection, \"$k$\")\n",
    "- How do we \"practice\" machine learning? What are the principles of good model-building?\\\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f561311b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Outline\n",
    "1. Feature Engineering\n",
    "2. The Bias-Variance Trade-Off\n",
    "3. Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ad8d31",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c6c00d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Feature Spaces\n",
    "- The data that we get is just raw material: We have to deliberately create a **feature space** that allows models to be expressive and clever about using the data\n",
    "- Already, we've seen various useful ways of processing the data: Log/IHS transformations, maxmin scaling, one-hot encoding\n",
    "- When we create/transform new variables, we create new information that was previously unavailable, expanding the space of models that we can estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4c9d9f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exercise\n",
    "- Pick a dataset with a numeric outcome and some promising variables/features\n",
    "- Discuss how you think those variables determine the target/outcome, in general\n",
    "- We're then going to talk about strategies to build a feature space in general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ff04f2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "da97cd6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age at Diagnosis</th>\n",
       "      <th>Type of Breast Surgery</th>\n",
       "      <th>Cancer Type</th>\n",
       "      <th>Chemotherapy</th>\n",
       "      <th>Hormone Therapy</th>\n",
       "      <th>Lymph nodes examined positive</th>\n",
       "      <th>Mutation Count</th>\n",
       "      <th>Nottingham prognostic index</th>\n",
       "      <th>Overall Survival (Months)</th>\n",
       "      <th>Overall Survival Status</th>\n",
       "      <th>Radio Therapy</th>\n",
       "      <th>TMB (nonsynonymous)</th>\n",
       "      <th>Tumor Size</th>\n",
       "      <th>Tumor Stage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>43.19</td>\n",
       "      <td>BREAST CONSERVING</td>\n",
       "      <td>Breast Cancer</td>\n",
       "      <td>NO</td>\n",
       "      <td>YES</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.020</td>\n",
       "      <td>84.633333</td>\n",
       "      <td>0:LIVING</td>\n",
       "      <td>YES</td>\n",
       "      <td>2.615035</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>48.87</td>\n",
       "      <td>MASTECTOMY</td>\n",
       "      <td>Breast Cancer</td>\n",
       "      <td>YES</td>\n",
       "      <td>YES</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.030</td>\n",
       "      <td>163.700000</td>\n",
       "      <td>1:DECEASED</td>\n",
       "      <td>NO</td>\n",
       "      <td>2.615035</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>47.68</td>\n",
       "      <td>MASTECTOMY</td>\n",
       "      <td>Breast Cancer</td>\n",
       "      <td>YES</td>\n",
       "      <td>YES</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.050</td>\n",
       "      <td>164.933333</td>\n",
       "      <td>0:LIVING</td>\n",
       "      <td>YES</td>\n",
       "      <td>1.307518</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>76.97</td>\n",
       "      <td>MASTECTOMY</td>\n",
       "      <td>Breast Cancer</td>\n",
       "      <td>YES</td>\n",
       "      <td>YES</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.080</td>\n",
       "      <td>41.366667</td>\n",
       "      <td>1:DECEASED</td>\n",
       "      <td>YES</td>\n",
       "      <td>2.615035</td>\n",
       "      <td>40.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>78.77</td>\n",
       "      <td>MASTECTOMY</td>\n",
       "      <td>Breast Cancer</td>\n",
       "      <td>NO</td>\n",
       "      <td>YES</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.062</td>\n",
       "      <td>7.800000</td>\n",
       "      <td>1:DECEASED</td>\n",
       "      <td>YES</td>\n",
       "      <td>5.230071</td>\n",
       "      <td>31.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age at Diagnosis Type of Breast Surgery    Cancer Type Chemotherapy  \\\n",
       "0             43.19      BREAST CONSERVING  Breast Cancer           NO   \n",
       "1             48.87             MASTECTOMY  Breast Cancer          YES   \n",
       "2             47.68             MASTECTOMY  Breast Cancer          YES   \n",
       "3             76.97             MASTECTOMY  Breast Cancer          YES   \n",
       "4             78.77             MASTECTOMY  Breast Cancer           NO   \n",
       "\n",
       "  Hormone Therapy  Lymph nodes examined positive  Mutation Count  \\\n",
       "0             YES                            0.0             2.0   \n",
       "1             YES                            1.0             2.0   \n",
       "2             YES                            3.0             1.0   \n",
       "3             YES                            8.0             2.0   \n",
       "4             YES                            0.0             4.0   \n",
       "\n",
       "   Nottingham prognostic index  Overall Survival (Months)  \\\n",
       "0                        4.020                  84.633333   \n",
       "1                        4.030                 163.700000   \n",
       "2                        4.050                 164.933333   \n",
       "3                        6.080                  41.366667   \n",
       "4                        4.062                   7.800000   \n",
       "\n",
       "  Overall Survival Status Radio Therapy  TMB (nonsynonymous)  Tumor Size  \\\n",
       "0                0:LIVING           YES             2.615035        10.0   \n",
       "1              1:DECEASED            NO             2.615035        15.0   \n",
       "2                0:LIVING           YES             1.307518        25.0   \n",
       "3              1:DECEASED           YES             2.615035        40.0   \n",
       "4              1:DECEASED           YES             5.230071        31.0   \n",
       "\n",
       "   Tumor Stage  \n",
       "0          1.0  \n",
       "1          2.0  \n",
       "2          2.0  \n",
       "3          2.0  \n",
       "4          4.0  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/metabric.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5228dcc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Age at Diagnosis', 'Type of Breast Surgery', 'Cancer Type',\n",
       "       'Chemotherapy', 'Hormone Therapy', 'Lymph nodes examined positive',\n",
       "       'Mutation Count', 'Nottingham prognostic index',\n",
       "       'Overall Survival (Months)', 'Overall Survival Status', 'Radio Therapy',\n",
       "       'TMB (nonsynonymous)', 'Tumor Size', 'Tumor Stage'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "390dfb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "vars = ['Age at Diagnosis', 'Type of Breast Surgery', 'Cancer Type',\n",
    "       'Chemotherapy', 'Hormone Therapy', 'Lymph nodes examined positive',\n",
    "       'Mutation Count', 'Nottingham prognostic index',\n",
    "       'Overall Survival (Months)', 'Overall Survival Status', 'Radio Therapy',\n",
    "       'TMB (nonsynonymous)', 'Tumor Size', 'Tumor Stage']\n",
    "vars_num = ['Age at Diagnosis', 'Tumor Size', 'Mutation Count','TMB (nonsynonymous)','Nottingham prognostic index']\n",
    "X_num = df.loc[:, vars_num]\n",
    "stage_ohc = pd.get_dummies(df[\"Tumor Stage\"], dtype = int, drop_first = True)\n",
    "chemo_ohc = pd.get_dummies(df[\"Chemotherapy\"], dtype = int, drop_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "83d6ed73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age at Diagnosis</th>\n",
       "      <th>Tumor Size</th>\n",
       "      <th>Mutation Count</th>\n",
       "      <th>TMB (nonsynonymous)</th>\n",
       "      <th>Nottingham prognostic index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>43.19</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.615035</td>\n",
       "      <td>4.020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>48.87</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.615035</td>\n",
       "      <td>4.030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>47.68</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.307518</td>\n",
       "      <td>4.050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>76.97</td>\n",
       "      <td>40.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.615035</td>\n",
       "      <td>6.080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>78.77</td>\n",
       "      <td>31.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.230071</td>\n",
       "      <td>4.062</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age at Diagnosis  Tumor Size  Mutation Count  TMB (nonsynonymous)  \\\n",
       "0             43.19        10.0             2.0             2.615035   \n",
       "1             48.87        15.0             2.0             2.615035   \n",
       "2             47.68        25.0             1.0             1.307518   \n",
       "3             76.97        40.0             2.0             2.615035   \n",
       "4             78.77        31.0             4.0             5.230071   \n",
       "\n",
       "   Nottingham prognostic index  \n",
       "0                        4.020  \n",
       "1                        4.030  \n",
       "2                        4.050  \n",
       "3                        6.080  \n",
       "4                        4.062  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_num.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dca34b53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age at Diagnosis</th>\n",
       "      <th>Tumor Size</th>\n",
       "      <th>Mutation Count</th>\n",
       "      <th>TMB (nonsynonymous)</th>\n",
       "      <th>Nottingham prognostic index</th>\n",
       "      <th>YES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>43.19</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.615035</td>\n",
       "      <td>4.020</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>48.87</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.615035</td>\n",
       "      <td>4.030</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>47.68</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.307518</td>\n",
       "      <td>4.050</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>76.97</td>\n",
       "      <td>40.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.615035</td>\n",
       "      <td>6.080</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>78.77</td>\n",
       "      <td>31.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.230071</td>\n",
       "      <td>4.062</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age at Diagnosis  Tumor Size  Mutation Count  TMB (nonsynonymous)  \\\n",
       "0             43.19        10.0             2.0             2.615035   \n",
       "1             48.87        15.0             2.0             2.615035   \n",
       "2             47.68        25.0             1.0             1.307518   \n",
       "3             76.97        40.0             2.0             2.615035   \n",
       "4             78.77        31.0             4.0             5.230071   \n",
       "\n",
       "   Nottingham prognostic index  YES  \n",
       "0                        4.020    0  \n",
       "1                        4.030    1  \n",
       "2                        4.050    1  \n",
       "3                        6.080    1  \n",
       "4                        4.062    0  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.concat([X_num, chemo_ohc], axis = 1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ccf74d7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -1.40105318,  -0.56330518,  -7.96169211,   7.05273172,\n",
       "       -11.62816492, -29.90857962])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "y = df['Overall Survival (Months)']\n",
    "model = LinearRegression()\n",
    "model = model.fit(X,y)\n",
    "\n",
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e39b884e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.60222163e+00, -4.36510364e-01,  1.77006531e+02, -1.32086302e+02,\n",
       "        3.15587766e+00, -3.54670305e+01, -4.74239015e-02,  1.47891950e-03,\n",
       "       -3.35622839e+00,  2.55829912e+00,  2.07805143e-01, -1.79695646e-02,\n",
       "        8.74607321e-03,  2.57272557e+00, -1.95646885e+00, -2.91442030e-01,\n",
       "        4.29562015e-01, -2.40034157e+01,  3.99003975e+01, -7.19787593e+00,\n",
       "       -7.08505477e+01, -1.64966262e+01,  5.16087643e+00,  5.26057343e+01,\n",
       "       -2.28744457e+00,  1.00457518e+01, -3.54670305e+01])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "y = df['Overall Survival (Months)']\n",
    "model = LinearRegression()\n",
    "model = model.fit(X_poly,y)\n",
    "\n",
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b651b403",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Age at Diagnosis', 'Tumor Size', 'Mutation Count',\n",
       "       'TMB (nonsynonymous)', 'Nottingham prognostic index', 'YES'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d48a2c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## We've already seen...\n",
    "- Our most useful transformation is probably the natural logarithm (or inverse hyperbolic since when there are zeros or negative values): Converts variables with long tails into more bell-shaped ones\n",
    "- One Hot Encoding: Transform one categorical/qualitative variable with $K$ labels into $K$ 0/1 variables that jointly encode the original label (remember to drop one label to avoid the dummy variable trap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd1bb23",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Normalization/Standardization\n",
    "- Especially with neighbor-based models and neural networks, normalizing the variables is important for computational stability\n",
    "- Maxmin: \n",
    "$$\n",
    "u_i = \\frac{x_i - \\min(x)}{\\max(x)-\\min(x)}\n",
    "$$\n",
    "- $z$-score: If all the variables are roughly bell-shaped (or are, after a log/ihs transformation), we can center them around 0 and give them a standard deviation of 1,\n",
    "$$\n",
    "z_i = \\frac{x_i - \\bar{x} }{s(x)}\n",
    "$$\n",
    "- Robust scaling: If there are outliers that make $z$-scaling unreliable,\n",
    "$$\n",
    "r_i = \\frac{x_i - \\text{median}(x)}{IQR(x)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0c1fdb",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Missing Value Dummies\n",
    "- For regression models, the following approach works very well:\n",
    "\n",
    "1. Create a missing value dummy, `df['var_na'] = df['var'].isna()`\n",
    "2. Replace missings with zero, `df['var_0'] = df['var'].nafill(0)`\n",
    "3. Including both `var_na` and `var_0` in any regression\n",
    "\n",
    "Why does this work?\n",
    "\n",
    "$$\n",
    "... + b_{\\text{var}_{NA}} \\text{var}_{NA} + b_{\\text{var}_0} \\text{var}_0+... = \\begin{cases}\n",
    "\\beta_{\\text{var}_0} \\text{var}_0, & \\text{ data available }\\\\\n",
    "\\beta_{\\text{var}_{NA}}, & \\text{ data missing }\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "- So the model smoothly switches back and forth between a linear model when data are available, and a missing data dummy when it's not available\n",
    "- **Only do this with models based on linear combinations of variables**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab851a6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Polynomial Expansion\n",
    "- In math, there's a fundamental concept (Stone-Weierstrass Theorem), that can be summarized like this: Any continuous function $\\mu(x)$ can be approximated arbitrarily well by polynomial functions of the form\n",
    "\\begin{alignat*}{2}\n",
    "m(x,b) &=& \\underbrace{b_0}_{\\text{0-order, constant, scalar}} + \\underbrace{\\sum_{\\ell = 1}^L b_\\ell x_\\ell}_{\\text{First-order, linear, vector}} \\\\\n",
    "&& + \\underbrace{\\sum_{\\ell =1}^L \\sum_{j=1}^L b_{\\ell j} x_\\ell x_j}_{\\text{Second-order, quadratic, matrix}} + \\underbrace{\\sum_{\\ell =1}^L \\sum_{j=1}^L \\sum_{p=1}^L b_{\\ell j p} x_\\ell x_j x_p}_{\\text{Third-order, quartic, tensor}} + \\underbrace{...}_{\\text{Higher order terms}} ,\n",
    "\\end{alignat*}\n",
    "in the sense that $\\max_{x} \\min_{b} | \\mu(x) - m(x,b) |$ can be made as small as desired by adding more higher order terms and optimizing over $b$\n",
    "- This motivates our interest in **linear models**: We can approximate any **known** function $\\mu(x)$ arbitrarily well with a model $m(x,b)$ where the $b$ coefficients are multiplicative with the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68260572",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Polynomial Expansion in Scikit\n",
    "\n",
    "1. **Import Expander**: `from sklearn.preprocessing import PolynomialFeatures`\n",
    "2. **Create Expander**: `expander = PolynomialFeatures(degree=2,include_bias=False) # Create the expander`\n",
    "3. **Fit Expander**: `X_poly = expander.fit_transform(X) `\n",
    "4. **Store variable names**: `poly_manes = expander.get_feature_names_out() `\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "163d9a2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[43.19    , 10.      ,  2.      , ..., 16.1604  ,  0.      ,\n",
       "         0.      ],\n",
       "       [48.87    , 15.      ,  2.      , ..., 16.2409  ,  4.03    ,\n",
       "         1.      ],\n",
       "       [47.68    , 25.      ,  1.      , ..., 16.4025  ,  4.05    ,\n",
       "         1.      ],\n",
       "       ...,\n",
       "       [52.84    , 20.      ,  5.      , ..., 25.4016  ,  5.04    ,\n",
       "         1.      ],\n",
       "       [48.59    , 30.      ,  6.      , ..., 25.6036  ,  5.06    ,\n",
       "         1.      ],\n",
       "       [63.2     , 22.      ,  3.      , ...,  9.265936,  0.      ,\n",
       "         0.      ]], shape=(1343, 27))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "expander = PolynomialFeatures(degree=2,include_bias=False) # Create the expander\n",
    "X_poly = expander.fit_transform(X)\n",
    "X_poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "40151965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Age at Diagnosis', 'Tumor Size', 'Mutation Count',\n",
       "       'TMB (nonsynonymous)', 'Nottingham prognostic index', 'YES',\n",
       "       'Age at Diagnosis^2', 'Age at Diagnosis Tumor Size',\n",
       "       'Age at Diagnosis Mutation Count',\n",
       "       'Age at Diagnosis TMB (nonsynonymous)',\n",
       "       'Age at Diagnosis Nottingham prognostic index',\n",
       "       'Age at Diagnosis YES', 'Tumor Size^2',\n",
       "       'Tumor Size Mutation Count', 'Tumor Size TMB (nonsynonymous)',\n",
       "       'Tumor Size Nottingham prognostic index', 'Tumor Size YES',\n",
       "       'Mutation Count^2', 'Mutation Count TMB (nonsynonymous)',\n",
       "       'Mutation Count Nottingham prognostic index', 'Mutation Count YES',\n",
       "       'TMB (nonsynonymous)^2',\n",
       "       'TMB (nonsynonymous) Nottingham prognostic index',\n",
       "       'TMB (nonsynonymous) YES', 'Nottingham prognostic index^2',\n",
       "       'Nottingham prognostic index YES', 'YES^2'], dtype=object)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expander.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd5a3a2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Interaction Terms\n",
    "- Among the terms of the polynomial expansion we are most interested in, are terms of the type\n",
    "$$\n",
    "b_{jk} x_{j}x_{k}, \\quad j \\neq k\n",
    "$$\n",
    "or\n",
    "$$\n",
    "b_{jk\\ell} x_{j}x_{k}x_{\\ell}, \\quad j \\neq k \\neq \\ell\n",
    "$$\n",
    "which capture the joint non-linear impact of changes in $x_j$ and $x_k$ or $x_\\ell$ together. \n",
    "- For example, the difference in price between a BMW with a sunroof versus without might be larger than the difference in price between a Honda with a sunfroof versus without. The BMW might exhibit more \"luxury bang-for-buck\" effects than more budget-conscious vehicles.\n",
    "- Use `interaction_only=True` in Scikit's polynomial expander to get only the interactions, if you want to avoid high order powers of the variables alone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f7fbc7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Temporal Features/Controls (Optional)\n",
    "- Some features/controls are periodic or recurrent, and a linear model seems like a bad fit: Think about hours of the day\n",
    "- In cases like this, we can do two things\n",
    "\n",
    "1. Bin the time variable into a categorical and one-hot encode (e.g. each hour of the day gets its own coefficient in the regression)\n",
    "2. Use a Fourier Series:\n",
    "$$\n",
    "m(t,b) = b_0 + \\sum_{\\ell=1}^L \\left[ b_{\\ell,\\cos} \\cos \\left(2 \\pi \\ell \\dfrac{ t}{T} \\right) + b_{\\ell,\\sin} \\sin \\left( 2 \\pi \\ell \\dfrac{t}{T} \\right) \\right],\n",
    "$$\n",
    "where $T$ is the period length ($T=24$ for hours, $T=7$ for day-of-week, $T=365$ for day-of-year, etc.)\n",
    "\n",
    "The Fourier Series is still a linear model in the $b$ coefficients, just like the polynomial family, but it's built from sines and cosines, so it behaves periodically and continuously as $t/T$ ranges over its possible values. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e451fc1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Fourier Series Basis Functions (Optional)\n",
    "- These are like polynomial families $1, x, x^2, x^3,...$, but for variation over time:\n",
    "<img src=\"./src/fourier.png\" alt=\"Illustration of bias and variance.\" width=\"100%\" height=\"auto\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a617f5d8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Time Series: Temporal Target/Outcome (Optional)\n",
    "- Imagine your target variable is denominated in time, so it's the $y_{it}$ rather than $x_{it}$ that is temporal\n",
    "- Instead of studying $y_{it}$ directly, take the **first difference**\n",
    "$$\n",
    "z_t = y_t - y_{t-1}\n",
    "$$\n",
    "or compute the **log growth** or **log return** (particularly in fenance)\n",
    "$$\n",
    "r_t = \\log \\left( \\frac{ y_t }{ y_{t-1} } \\right)\n",
    "$$\n",
    "- In regression, we often include **lags** of the variable to account for its \"momentum\" or \"trajectory\" over time, like\n",
    "$$\n",
    "\\hat{z}_t = b_0 + b_1 z_{t-1} + b_2 z_{t-2} + ... + b_K z_{t-K}\n",
    "$$\n",
    "So we model the variable's evolution over time based on recent changes in its values, rather than levels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0dd177",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exercise\n",
    "- Engineer a feature space for your data\n",
    "- Do a train-test split\n",
    "- Train a simple linear model on the training data\n",
    "- Train a complex linear model on the training data\n",
    "- Which does better (lower MSE) on the test set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a3a991",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 2. The Bias-Variance Trade-Off"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570c8b6c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Machine Learning\n",
    "- Many fields are driven by a handful of core ideas\n",
    "- For machine learning/data science, one of these core ideas is called the **bias-variance trade-off**\n",
    "- This is a mathematical concept and we'll briskly cover the quantitative argument behind it, and then focus on the intuition it provides\n",
    "- This discussion is mathematical, to show how you discover and think about an idea like this\n",
    "- This lesson is essentially the \"general theory of machine learning\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49fdad5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Expectation\n",
    "- One of our favorite quantities is the sample mean of a variable $X$,\n",
    "$$\n",
    "m(X) = \\sum_{i=1}^N \\frac{1}{n} x_i\n",
    "$$\n",
    "- We want to imagine a similar concept: What value of $X$ is most likely, before we've observed the data?\n",
    "- We are going to replace $1/n$ with the abstract probability that $X=x$ occurs, $p(x)$, and sum over possible values of $X$\n",
    "- The **expectation of a random variable $X$** is\n",
    "$$\n",
    "\\mathbb{E}_X[X] = \\sum_{\\text{All } x \\text{ such that }p(x)>0} p(x) x\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e82eee",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Expectation\n",
    "- Another way to understand this: Imagine computing $m(X)$ by first computing the histogram, and weighting each $x_i$ by the height of the histogram\n",
    "- Another way to understand this: It answers the question, \"Before we actually gather any data, what do you expect the sample mean to be?\"\n",
    "- As data scientists we love this: It's like we're thinking about the model before we actually gather data or do any analysis\n",
    "- Key observation: $\\mathbb{E}_X[X]$ is just a number, like 7.46. So $\\mathbb{E}_X[ \\mathbb{E}_X[X] ] = \\mathbb{E}_X[X]$ (idempotent operator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e493a60",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Expectation: Example\n",
    "- What is the expected value of rolling a single die?\n",
    "- What is the expected value of rolling two dice and summing the results?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1fd878",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bias and Model Variance\n",
    "- There are two properties of an estimate that it's important to keep in mind:\n",
    "\n",
    "1. **Bias**: If the truth is $T$ and we predicted $\\hat{T}$, the bias is equal to $\\mathbb{E}_{\\hat{T}}[\\hat{T} - T]$, the expected difference between the true value and out predicted value\n",
    "2. **Variance**: The variability of our prediction is $\\mathbb{E}_{\\hat{T}}[ (\\hat{T}-T)^2 ]$\n",
    "\n",
    "We obviously don't like bias: If it is large, we make lots of mistakes in expectation. But it turns out that we also don't like variance: If the variance is large, our predictions are typically unreliable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb41f6a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bias and Variance: Classic Picture\n",
    "\n",
    "<img src=\"./src/bvt.png\" alt=\"Illustration of bias and variance.\" width=\"65%\" height=\"auto\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c138e37",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bias-Variance Trade-Off\n",
    "- It turns out that bias and variance are the core considerations of how we build models\n",
    "- There's a bunch of features/covariates $x_1, x_2, ..., x_L$, which jointly determine the expected value of $y$, which is $\\mathbb{E}_X[y|X] = \\mu(x_1, x_2, ..., x_L)$\n",
    "- However, there's an additive shock, $\\varepsilon$, with mean zero, so we never observe $\\mu$ directly:\n",
    "$$\n",
    "y(x_1, x_2, ..., x_L) = \\mu(x_1, x_2, ..., x_L) + \\varepsilon\n",
    "$$\n",
    "- We assume the expected value of $\\varepsilon$ is $\\mathbb{E}[\\varepsilon] = 0$ and its variance is $\\mathbb{E}[\\varepsilon^2] = \\sigma^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f481465",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bias-Variance Trade-Off\n",
    "- Given this set-up, what is machine learning?\n",
    "- What we're doing is guessing $\\mu(x_1, x_2, ..., x_L)$ with a model $m(x_1, x_2, ..., x_L; b)$, and then choosing $b$ to solve this:\n",
    "$$\n",
    "\\min_{b} \\frac{1}{n} \\sum_{i=1}^n (\\mu(x_i) + \\varepsilon_i - m(x_i,b))^2\n",
    "$$\n",
    "- Our model, $m(x,b)$ is just a model: $k$-NN, linear, whatever we want."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae8c5cb",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Expected Loss\n",
    "- **Before** we gather the data, what do we **expect**ed to happen with our loss function (MSE)?\n",
    "\\begin{alignat*}{2}\n",
    "\\mathbb{E}_{X,\\varepsilon}\\left[ \\frac{1}{n} \\sum_{i=1}^n (\\mu(x_i) + \\varepsilon_i - m(x_i,b))^2 \\right] \n",
    "\\end{alignat*}\n",
    "- Let's work out how to simplify this There's essentially two tricks: FOIL and simplify, then add/subtract and FOIL again, using the expected value to erase a surprising number of terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15445988",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bias-Variance Trade-Off\n",
    "- So our MSE breaks into three distinct pieces:\n",
    "\\begin{alignat*}{2}\n",
    "\\mathbb{E}_{X,\\varepsilon}\\left[ \\text{MSE}(b) \\right] &=& \\underbrace{\\mathbb{E}_{X,\\varepsilon}\\left[ \\frac{1}{n} \\sum_{i=1}^n  (\\mu(x_i) - \\mathbb{E}_{X}[m(X,b)])^2 \\right]}_{\\text{Predictor Bias}} + \\underbrace{\\mathbb{E}_{X,\\varepsilon}\\left[ \\frac{1}{n} \\sum_{i=1}^n (\\mathbb{E}_{X}[m(X,b)]- m(x_i,b))^2 \\right]}_{\\text{Model Variance}}\\\\\n",
    "&& + \\underbrace{\\sigma^2}_{\\text{Irreducible Error}}\n",
    "\\end{alignat*}\n",
    "and the take-away intuition from this equation is that\n",
    "$$\n",
    "\\text{Expected Loss} = \\text{Bias-Squared} + \\text{Model Variance} + \\text{Irreducible Error}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870c2ed5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bias-Variance Trade-Off\n",
    "\n",
    "- So our expected loss decomposes into three pieces:\n",
    "    1. **Predictor Bias**: In expectation, how far do we expect our model $m$ to be from the true $\\mu$?\n",
    "    2. **Model Variance**: Regardless of the truth, how variable is our model in expectation?\n",
    "    3. **Irreducible Error**: How much inherent noise is there in the environment?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e761927",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## BVT for $k$-NN\n",
    "- For the $k$-neighbors regressor model, there's an explicit formula:\n",
    "$$\n",
    "\\mathbb{E}_{X,y}[(y-m(x))^2|X=x] = \\underbrace{\\left( \\mu(x) - \\frac{1}{k}\\sum_{\\text{$i$ in Neighbors}(x)}^K y_i \\right)^2}_{\\text{Bias squared}} + \\underbrace{\\frac{\\sigma^2}{k}}_{\\text{Variance}} + \\underbrace{\\sigma^2}_{\\text{Irreducible Error}}\n",
    "$$\n",
    "- As $k$ increases, the model variance decreases: You are averaging over more and more neighbors, so the predictor variance shrinks\n",
    "- As $k$ increases, the bias squared increases: You are averaging over cases that are further and further from $x$, so in expectation, they are becoming less relevant\n",
    "- This is the BVT in practice: A very low $k$ and a very high $k$ are typically both not the solution, and the optimal $k$ depends on the prediction environment in question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656eb94d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Optimal Model Complexity\n",
    "\n",
    "<img src=\"./src/model_complexity.png\" alt=\"Illustration of model complexity trade-off.\" width=\"75%\" height=\"auto\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a372c165",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Over-fitting, Under-fitting\n",
    "- Excessively complex models are associated with high variance but low bias, leading to high expected loss: This is called **over-fitting**\n",
    "- Excessively simplistic models are associated with low variance but high bias, leading to high expected loss: This is called **under-fitting**\n",
    "- We want to balance **parsimony** with **verisimilitude**, and avoid both under- and over-fitting\n",
    "- This is what the train-test split is about: Using data to judge when the model is transitioning from simple and robust to overly complicted and unreliable\n",
    "- This requires judgment and appropriate use of model selection tools\n",
    "- This is a fundamental concept that we'll talk about for the rest of class, and the kind of idea you should commit to your data science core archive of ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed89b40",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3. Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681510f5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Model Selection and Validation\n",
    "- Ok, we can build massive and complex feature spaces...\n",
    "- ... but we know this will typically lead to overfitting and needlessly complex models that aren't BVT-optimal\n",
    "- How do we estimate expected performance, so we can address concerns about the bias-variance trade-off?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b07b7c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exercise\n",
    "- For your data, pick a model selection/hyperparameter choice about which you are uncertain\n",
    "- For example, \"How many powers of $x$ should I include?\", \"How many lags of the time series should I use?\", \"Should I include this variable or not?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7579a017",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## $K$-Fold Cross Validation\n",
    "- This is a kind of \"$K$-way train/test split\":\n",
    "\n",
    "0. Fix the model (the variables, the neighbors, how many polynomial features, etc.)\n",
    "1. Partition the data into $K$ equally-sized sets (the folds)\n",
    "2. For each $k=1,...,K$, estimate your model's parameters on the complementary chunks, and test performance on the the $k$-th chunk\n",
    "3. Save the model's performance for each chunk (e.g. accuracy, MSE)\n",
    "\n",
    "This provides $K$ estimates of the model's performance, estimating the distribution function of the model's performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8876ddf9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<img src=\"./src/crossvalidation.png\" alt=\"Illustration of $k$-Fold Cross Validation.\" width=\"80%\" height=\"auto\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f54ce0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Scikit Implementation: Mostly Automatic\n",
    "\n",
    "``` python \n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "[...]\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=100) # Create folds\n",
    "\n",
    "scores = cross_val_score( # Conduct kfcv:\n",
    "    model,X,y, # Model and data\n",
    "    cv=kfold, # Folds\n",
    "    scoring=mean_squared_error # Loss function\n",
    ")\n",
    "\n",
    "print(\"Fold scores:\", scores)\n",
    "print(\"Mean score:\", np.mean(scores))\n",
    "print(\"Median score:\", np.median(scores))\n",
    "print(\"Std dev:\", np.std(scores))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c298c28",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Scikit Implementation: More Control\n",
    "\n",
    "``` python \n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "[...]\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "scores = []\n",
    "for train_idx, test_idx in kf.split(X):\n",
    "    X_train, X_test = X[train_idx], X[val_idx]\n",
    "    y_train, y_test = y[train_idx], y[val_idx]\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_hat = model.predict(X_test)\n",
    "    score = mean_squared_error(y_test, y_hat)\n",
    "    scores.append(score)\n",
    "\n",
    "print(\"Fold scores:\", scores)\n",
    "print(\"Mean score:\", np.mean(scores))\n",
    "print(\"Median score:\", np.median(scores))\n",
    "print(\"Std dev:\", np.std(scores))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9f3ef9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exercise\n",
    "- For your model selection question, use $k$-FCV to estimate the model's performance for a variety of plausible alternative, and pick the alternative with the best median or average performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d778c8a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusion\n",
    "- This was a lot! \n",
    "- Feature engineering is essential to build realistic, expressive models\n",
    "- But we can go too far, and end up over-fitting, or not far enough, and under-fit: this is the essence of the BVT\n",
    "- Cross validation allows us to quantify whether we're doing a good job or not\n",
    "- These are fundamental topics in the field of ML/DS\n",
    "- The connections with probability and simulation can be further developed, but getting some experience with the ideas first is a good idea\n",
    "- While we're here: The point of deep learning is to use a neural network to automatically do feature engineering for us (representation learning)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
